job.parallelism = 1
spark.checkpoints.interval = "20 minutes"
application.name = "spark_enrichment_app"
checkpointLocation = "hdfs://user/asdolgaya/spark_checkpoints"
source = {
  name = "kafka_source"
  format = "kafka"
  subscribe = "test3"
  kafka.bootstrap.servers = "kafka:9092"
  group_id = "spark-group"
  failOnDataLoss = "false"
  scan.startup.mode = "earliest-offset"
}
ms_ip = {
  format = "jdbc"
  url = "jdbc:postgresql://host.docker.internal:5432/diploma"
  dbtable = "public.ms_ip"
  user = "postgres"
  password = "7844"
  cache.initial_delay_min = 0
  cache.period_min = 5
}

imsi_msisdn = {
  format = "jdbc"
  url = "jdbc:postgresql://host.docker.internal:5432/diploma"
  dbtable = "public.imsi_msisdn"
  user = "postgres"
  password = "7844"
  cache.initial_delay_min = 0
  cache.period_min = 5
}

sink = {

  hdfs = {
    name = "hdfs_sink"
    partitionBy = ["event_date", "probe"]
    blockSizeMB = 128
    format = "parquet"
    path = "hdfs://namenode:8020/spark/results"
    checkpointLocation = "hdfs://namenode:8020/spark/checkpoints"
  }
}
